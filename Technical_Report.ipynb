{"cells":[{"cellId":"6dfcb49bfc8e43f89a0d887476a45e91","cell_type":"markdown","metadata":{"id":"ZfBzaipSlnuq","deepnote_app_block_order":0,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"6dfcb49bfc8e43f89a0d887476a45e91","deepnote_cell_type":"markdown"},"source":"<center>\n\n# **Curso: Redes Neuronales Artificiales y Algoritmos Bioinspirados**\n\n---\n\n## ***Semestre: 2025 - 1***\n\n---\n\n## ***Trabajo #2: Redes Neuronales Aplicadas a Datos Tabulares***\n\n---\n\n### **Equipo #2**:\n\nCarlos José Quijano Valencia\n<br>\nMiller Alexis Quintero García\n<br>\nKelly Yojana Ospina Correa\n<br>\nMateo Sebastián Mora Montero\n<br>\nStiven Julio Doval\n\n<br>\n\n**Profesor:** Juan David Ospina Arango  \n**Monitor:** Andrés Mauricio Zapata Rincón\n\n<br>\n\n**Universidad Nacional de Colombia, Sede Medellín**\n\n***Junio 12 del 2025***\n\n**Enlace a repositorio GitHub:** <https://github.com/Straver00/RNA_2025-1>\n\n**Enlace Notebook Preprocesamiento:**  <https://colab.research.google.com/drive/1_tcjWV1vsKgS0-hYzu6FKs_UqTkYx035?usp=sharing>\n\n**Enlace Notebook Experimental de Modelos** <https://colab.research.google.com/drive/1P3ZvN-ow3tYFBMKE_h0_MqI-XkFv4d2a?usp=sharing>\n</center>","block_group":"00571e83685944489c11851de78a32d1"},{"cellId":"46e20fb344434392ae441c54e5999239","cell_type":"markdown","metadata":{"id":"wiXV6U5dmUfF","deepnote_app_block_order":1,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"46e20fb344434392ae441c54e5999239","deepnote_cell_type":"markdown"},"source":"# Tabla de contenidos\n\n1.  [Introducción](#introduccion)\n2.  [Delimitación y Metodología](#delimitacion-y-metodologia)\n3.  [Limpieza de datos](#limpieza-de-datos)\n4.  [Análisis Descriptivo e Hipótesis](#analisis-descriptivo-e-hipotesis)\n5.  [Modelos](#modelos)\n    - [Definición de Métricas Claves](#definicion-de-metricas-claves)\n    - [Tratamiento del Desbalance](#tratamiento-del-desbalance)\n    - [Función de Pérdida](#funcion-de-perdida)\n    - [Estrategia de Entrenamiento](#estrategia-de-entrenamiento)\n    - [Perceptrones Simples como Referencia](#perceptrones-simples-como-referencia)\n    - [Redes Neuronales Sencillas](#redes-neuronales-sencillas)\n    - [Red con Batch Normalization y Dropout](#red-con-batch-normalization-y-dropout)\n    - [Selección del Umbral de Corte y Modelo](#seleccion-del-umbral-de-corte-y-modelo)\n    - [Scorecard para el Modelo Final](#scorecard-para-el-modelo-final)\n6.  [Aplicación web](#aplicacion-web)\n7.  [Bibliografía](#bibliografia)","block_group":"050a91295ab54776a8af764825a083b0"},{"cellId":"786e7e549cb44b57a8497a036287c2b3","cell_type":"markdown","metadata":{"id":"YR0ylaRItxc1","deepnote_app_block_order":2,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"786e7e549cb44b57a8497a036287c2b3","deepnote_cell_type":"markdown"},"source":"# Introduccion\n\nEl presente trabajo se enfoca en el desarrollo y evaluación de modelos de redes neuronales artificiales aplicados a datos tabulares, con el objetivo de proporcionar una herramienta eficaz para la predicción y clasificación de datos en un contexto práctico.\n\nEl propósito principal del trabajo es aplicar técnicas avanzadas de redes neuronales para resolver problemas reales de clasificación. En particular, nos hemos enfocado en la creación de un modelo predictivo que permita a los usuarios evaluar la viabilidad de obtener créditos, basado en características que son fácilmente accesibles por el individuo y que no requieren información confidencial proveniente de instituciones bancarias.\n\nA lo largo de este informe, se detallarán las metodologías empleadas en el preprocesamiento de los datos, la selección de modelos, así como los resultados obtenidos en las pruebas de clasificación. Además, se presentarán recomendaciones sobre las mejores prácticas y futuras mejoras en el campo de la inteligencia artificial aplicada a los datos tabulares.","block_group":"39b02caad53e41459db2a51017719114"},{"cellId":"93e5844566b142dab77bdc3a83a6b903","cell_type":"markdown","metadata":{"id":"xxgdLhUkt3-M","deepnote_app_block_order":3,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"93e5844566b142dab77bdc3a83a6b903","deepnote_cell_type":"markdown"},"source":"# Delimitacion y Metodologia\nEl objetivo de desarrollar el modelo se plantea en un contexto en cuál se debe desarrollar una app web para uso de personas. En ese sentido la delimitación inmediata que surge es que el app web debe tener un formulario, en el que la persona ingrese sus datos y así, conozca tanto su scorecard, como si puede o no acceder a crédito.\nConsiderando eso, lo primero será entender que el modelo debe recibir información y variables que una persona si pueda proporcionar fácilmente desde su conocimiento, y sin necesidad de pedir información que sea condifencial de entidades bancarias o burós de crédito.\nTras esa consideración, se entiende entonces que hay muchas variables que se podrán descarta.\n\nAsí pues un flujo estrategico a seguir es:\n\n- Limpieza de datos, descartando variables con criterios de imposibilidad de preguntar o no factibilidad de imputación por enormes proporciones de datos nulos.\n- Codificación de variables categóricas.\n- Análisis para obtener indicadores e ideas valiosas el dataset.\n- Escalamiento de variables numéricas continuas.\n- División de los datos en entrenamiento y prueba.\n- Entrenamiento de diversos modelos con técnicas variadas.\n- Selección del mejor modelo en base a métricas relacionadas con el problema de clasificación.\n- Desarrollo de la app web con el scorecard de la persona según predicciones del modelo.\n","block_group":"262fbadde41b4f6ba159279b1d35ed2e"},{"cellId":"a87be0087c3d40f2b75f4ef6c5b885c6","cell_type":"markdown","metadata":{"id":"SZJnA4Jev1H1","deepnote_app_block_order":4,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"a87be0087c3d40f2b75f4ef6c5b885c6","deepnote_cell_type":"markdown"},"source":"# Limpieza de datos \nEsta sección informa de manera detalla los pasos de limpieza y preprocesamiento de datos realizados en el conjunto de datos **loan.csv**. El objetivo principal de estos pasos es preparar los datos para un modelo de aprendizaje automático, asegurando la calidad de los datos, manejando los valores faltantes y transformando las variables a formatos adecuados.","block_group":"b51afd221f4c45d3a3697e5667ec6089"},{"cellId":"a3796dc0c20e421b97fc67a901c3bf8a","cell_type":"markdown","metadata":{"deepnote_app_block_order":5,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"a3796dc0c20e421b97fc67a901c3bf8a","deepnote_cell_type":"markdown"},"source":"## Definicion variable objetivo\n\nEl proceso comienza cargando el conjunto de datos **loan.csv** en un DataFrame de pandas. La columna **loan_status**, que es la variable objetivo para este análisis, contiene varios valores de cadena que representan el estado del préstamo. Para crear un problema de clasificación binaria (por ejemplo, predecir el incumplimiento del préstamo), estos valores se mapean a **0** (Totalmente Pagado) o **1** (Castigado/Atrasado/Incumplido) y se quitan los registros en donde no se tenga certeza del comportamiento final del prestatario.\n\nEsto asegura que la variable objetivo sea estrictamente binaria y esté en un formato numérico, lo cual es un requisito para la mayoría de los algoritmos de aprendizaje automático.","block_group":"0c586fc6a6aa493fbecd6956e552349c"},{"cellId":"d88c9171e499425e941f53e8b68768f5","cell_type":"markdown","metadata":{"deepnote_app_block_order":6,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"d88c9171e499425e941f53e8b68768f5","deepnote_cell_type":"markdown"},"source":"## Eliminación de variables","block_group":"1f708d14db00464f93eedcf354f42d4e"},{"cellId":"7a0a7b03f12d43a782d59b06678819b3","cell_type":"markdown","metadata":{"deepnote_app_block_order":7,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"7a0a7b03f12d43a782d59b06678819b3","deepnote_cell_type":"markdown"},"source":"### Variables con mas de la mitad de datos nulos\n\nEn este caso se identifican las 22 columnas principales con la mayor cantidad de datos faltantes y se eliminan.\n**Justificación**: Las columnas con un porcentaje muy alto de valores faltantes (por ejemplo, más del 50%) a menudo proporcionan poca información útil y pueden introducir ruido o sesgo en el modelo. Eliminarlas simplifica el conjunto de datos y mejora el rendimiento del modelo al eliminar características poco confiables.","block_group":"ed61a4b551bd491f925532d7149531f7"},{"cellId":"16e930f2e6e94490a852c51ab97281c4","cell_type":"markdown","metadata":{"deepnote_app_block_order":8,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"16e930f2e6e94490a852c51ab97281c4","deepnote_cell_type":"markdown"},"source":"### Variables descriptivas, textuales:\nEstas variables son típicamente identificadores únicos, texto de formato libre o fechas que no son directamente utilizables como características numéricas para la predicción u ofrecen información redundante. Es poco probable que contribuyan significativamente a predecir el incumplimiento del préstamo, es por esto que no se toman en cuenta para predecir la variable objetivo.\n\n- **id**: Una identificación única asignada por LC para la lista de préstamos.\n- **member_id**: Un ID único asignado por LC para el miembro prestatario.\n- **url**: URL de la página LC con datos del listado.\n- **title**: El título del préstamo proporcionado por el prestatario\n- **emp_title**: El título del puesto de trabajo proporcionado por el prestatario al solicitar el préstamo.\n- **zip_code**: Los primeros 3 números del código postal proporcionado por el prestatario en la solicitud de préstamo.\n- **issue_d**: El mes en el que se financió el préstamo\n- **addr_state**: El estado proporcionado por el prestatario en la solicitud de préstamo\n\n- **earliest_cr_line**: El mes en que se abrió la primera línea de crédito informada del prestatario","block_group":"5cc7b529877443c195f3ba4ddfa89665"},{"cellId":"19cd33f5581545de94c7e0ab555250de","cell_type":"markdown","metadata":{"deepnote_app_block_order":9,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"19cd33f5581545de94c7e0ab555250de","deepnote_cell_type":"markdown"},"source":"### Variables post solicitud:\n\nEstas variables representan información después de que el préstamo ha sido financiado o los pagos han comenzado. Incluirlas introduciría una fuga de datos (data leakage), ya que sus valores no se conocerían en el momento de predecir el incumplimiento del préstamo. El objetivo es predecir el incumplimiento antes de que ocurran estos eventos\n\n- **out_prncp**: Capital pendiente restante por el monto total financiado\n- **out_prncp_inv**: Capital pendiente restante correspondiente a la parte del importe total financiado por los inversores\n- **total_pymnt**: Pagos recibidos hasta la fecha por el monto total financiado\n- **total_pymnt_inv**: Pagos recibidos hasta la fecha por una parte del importe total financiado por los inversores\n- **total_rec_prncp**: Principal recibido hasta la fecha\n- **total_rec_int**: Intereses recibidos hasta la fecha\n- **total_rec_late_fee**: Cargos por pagos atrasados recibidos hasta la fecha\n- **recoveries**: recuperación bruta posterior a la cancelación de la deuda\n- **collection_recovery_fee**: Tarifa de cobro por cancelación de publicación\n- **last_pymnt_d**: Se recibió el pago del mes pasado:\n- **last_pymnt_amnt**: Último importe total de pago recibido\n- **last_credit_pull_d**: El mes más reciente LC retiró el crédito para este préstamo\n- **funded_amnt**: El monto total comprometido para ese préstamo en ese momento.\n- **funded_amnt_inv**: El monto total comprometido por los inversores para ese préstamo en ese momento.","block_group":"5173e8a54d0a4f07a0c9848b078e59f4"},{"cellId":"fba41f46a6fc4da397fc7956c2562305","cell_type":"markdown","metadata":{"deepnote_app_block_order":10,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"fba41f46a6fc4da397fc7956c2562305","deepnote_cell_type":"markdown"},"source":"### variables con muy poca variabilidad:\n\nLas variables con muy poca o ninguna varianza proporcionan información mínima o nula para un modelo predictivo. Si una variable tiene casi el mismo valor para todas las observaciones, no puede ayudar a diferenciar entre clases (por ejemplo, préstamos en incumplimiento versus préstamos no en incumplimiento).\n\n- **pymnt_plan**: Indica si se ha establecido un plan de pago para el préstamo.\n- **policy_code**: \"publicly available policy_code=1. new products not publicly available policy_code=2\"\n- **application_type**; Indica si el préstamo es una solicitud individual o una solicitud conjunta con dos co-prestatarios\n- **acc_now_delinq**: El número de cuentas en las que el prestatario se encuentra moroso actualmente.","block_group":"0de7bf6c7e474660a3a8564f851c0d68"},{"cellId":"b016bac751f2416a8952b04442238e34","cell_type":"markdown","metadata":{"deepnote_app_block_order":11,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"b016bac751f2416a8952b04442238e34","deepnote_cell_type":"markdown"},"source":"### Variables que no son preguntables al usuario:\n\nEste es un paso crucial para la aplicación práctica del modelo. Muchas de estas variables (por ejemplo, **grade**, **sub_grade**, **int_rate**, **installment**, **dti**, **revol_util**, ** tot_cur_bal** ) son asignaciones internas de LC, derivadas de informes de crédito, o son valores que el usuario promedio no conocería con precisión en el momento de solicitar un préstamo sin herramientas específicas o acceso a informes financieros detallados. El objetivo es construir un modelo que se base en la información disponible directamente a partir de la entrada del usuario, esto para funcionalidad e interaccion intuitiva de la pagina web del modelo.\n\n- **grade** y **sub_grade**: Grado y subgrado de préstamo asignado por LC.\n- **delinq_2yrs**: El número de incidencias de morosidad con más de 30 días de retraso en el historial crediticio del prestatario durante los últimos 2 años.\n- **inq_last_6mths**:El número de consultas en los últimos 6 meses (excluidas las consultas sobre automóviles e hipotecas)\n- **open_acc**: El número de líneas de crédito abiertas en el expediente crediticio del prestatario.\n- **pub_rec**:Número de registros públicos despectivos\n- **int_rate**: Tasa de interés del préstamo\n- **installment**: El pago mensual que debe el prestatario si se origina el préstamo.\n- **dti**: Una proporción calculada utilizando los pagos mensuales totales de la deuda del prestatario sobre las obligaciones totales de la deuda, excluida la hipoteca y el préstamo LC solicitado, dividido por el ingreso mensual declarado por el prestatario.\n- **revol_bal**: Saldo total del crédito rotatorio\n- **revol_until**: Tasa de utilización de la línea revolvente, o la cantidad de crédito que el prestatario está utilizando en relación con todo el crédito revolvente disponible.\n- **total_acc**: El número total de líneas de crédito actualmente en el expediente crediticio del prestatario\n- **collections_12_mths_ex_med**: Número de cobros en 12 meses excluyendo cobros médicos\n- **tot_coll_amt**: Montos totales de cobro alguna vez adeudados\n- **total_rev_hi_lim**: Crédito total rotatorio alto/límite de crédito\n- **initial_list_status**: Estado inicial de cotización del préstamo. Los valores posibles son: W, F\n- **tot_cur_bal**: Saldo corriente total de todas las cuentas, pues un usuario no suele conocer este dato con exactitud y requiere acceder a múltiples fuentes financiera, cosa que solo puede hacer un buró de crédito.\n","block_group":"5224fd51a89a4b36b354d5d2f1c4f09d"},{"cellId":"081d2d00472f4f06b8991126a8a4c082","cell_type":"markdown","metadata":{"deepnote_app_block_order":12,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"081d2d00472f4f06b8991126a8a4c082","deepnote_cell_type":"markdown"},"source":"## Codificacion rapida y eleccion de variables predictoras","block_group":"799d09e6839e40ed9b5f8c743f8bfc03"},{"cellId":"b058e22a32414c219140f3be48a2351a","cell_type":"markdown","metadata":{"deepnote_app_block_order":13,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"b058e22a32414c219140f3be48a2351a","deepnote_cell_type":"markdown"},"source":"### Codificación rápida:\nSe preparan mejor estas variables por comodidad e interpretacion\n\n- **verification_status**:  Indica si los ingresos del prestatario fueron verificados.\n\nSe codifica esta variable como verificado :1 y no verificado: -1 para el entrenamiento apropiado del modelo\n\n- **home_ownership**: El estado de propiedad de la vivienda proporcionado por el prestatario durante el registro. Nuestros valores son:  RENT, OWN, MORTGAGE, OTHER.\n\nSe descartan las filas con NONE o ANY por que se considera como una respuesta no lógica o ambigua a la pregunta además de tener pocos de estos valores.","block_group":"e2262f677817457b9cd1386d0243c44b"},{"cellId":"e942e1b617ec4b518ca86b700f9c0a5c","cell_type":"markdown","metadata":{"deepnote_app_block_order":14,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"e942e1b617ec4b518ca86b700f9c0a5c","deepnote_cell_type":"markdown"},"source":"### Posibles variables predictoras:\n- **loan_amnt**: El monto del préstamo solicitado por el prestatario. (Si en algún momento el departamento de crédito reduce el monto del préstamo, esto se reflejará en este valor).\n- **annual_inc**: El ingreso anual declarado por el prestatario durante el registro.\n- **emp_length**: Duración del empleo en años. Los valores posibles están entre 0 y 10, donde 0 significa menos de un año y 10 significa diez o más años.\n- **verification_status**:  Indica si los ingresos del prestatario fueron verificados.\n- **home_ownership**: El estado de propiedad de la vivienda proporcionado por el prestatario durante el registro. Nuestros valores son:  RENT, OWN, MORTGAGE, OTHER.\n- **purpose**: Una categoría proporcionada por el prestatario para la solicitud de préstamo.(DIFICIL) (verificar relaciones)","block_group":"dc0bb36fa9be4396ad28ca5d6d81e050"},{"cellId":"824b50d2bcc44964adef59d03f8c5252","cell_type":"markdown","metadata":{"deepnote_app_block_order":15,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"824b50d2bcc44964adef59d03f8c5252","deepnote_cell_type":"markdown"},"source":"## Imputación de datos faltantes\nSolo hacen falta datos en la columna **'emp_length'**, esta es categórica que indica la cantidad de años de que lleva empleada una persona, los valores nulos no los vamos ni a inputar ni eliminar en este caso, si no que crearemos un indicativo de ausencia de información, dando a entender que el usuario deliberadamente decidió no proporcionar la cantidad de años de empleo.Este enfoque reconoce que la ausencia de información en sí misma podría ser una señal significativa para el modelo, en lugar de asumir que es similar a otra categoría existente o simplemente descartar los datos.","block_group":"23dd36228abb4e6c992d77cd0408c2b7"},{"cellId":"f3173b0929dc46e6a1873b36b24e8a2a","cell_type":"markdown","metadata":{"deepnote_app_block_order":16,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"f3173b0929dc46e6a1873b36b24e8a2a","deepnote_cell_type":"markdown"},"source":"##  Codificacion One-hot\n- Se aplica la codificación One-Hot a las variables categoricas restantes, ya que convierte las variables categóricas a un formato booleano creando nuevas columnas para cada categoría. Por ejemplo, **home_ownership** con categorías 'RENT', 'OWN', 'MORTGAGE','OTHER' se transformaría en las columnas **home_ownership_RENT**,** home_ownership_OWN** y **home_ownership_MORTGAGE**, **home_ownership_OTHER** cada una conteniendo **TRUE** o **FALSE**.\n\n- Se elimina una columna redundante de cada conjunto de variables codificadas one-hot. Específicamente, se eliminan **term_ 60 months**, **emp_length_n/a**, **purpose_other** y **home_ownership_OTHER**. Esto se debe a que la información de una categoría es redundante si todas las demás son conocidas.  Por ejemplo, eliminar **home_ownership_OTHER** significa que si un prestatario no es 'MORTGAGE', 'OWN' o 'RENT', debe ser 'OTHER'.\n- Las columnas booleanas se identifican y se convierten a valores numéricos, mapeando **True** a 1 y **False** a -1. Esto asegura que las columnas booleanas, que podrían haberse creado durante la codificación one-hot o ya existían, también estén en un formato numérico adecuado para el entrenamiento del modelo.","block_group":"9a360a56d9da4292bc7f56be348a12ae"},{"cellId":"93ba46a979dc47f8bc074152ea93c2b6","cell_type":"markdown","metadata":{"deepnote_app_block_order":17,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"93ba46a979dc47f8bc074152ea93c2b6","deepnote_cell_type":"markdown"},"source":"## Preparación de datos para el modelo","block_group":"1c2c87cca638442f97f1b878a3fa2fb3"},{"cellId":"5f56f9ae27274aee91e3a822f8369259","cell_type":"markdown","metadata":{"deepnote_app_block_order":18,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"5f56f9ae27274aee91e3a822f8369259","deepnote_cell_type":"markdown"},"source":"### Reordenar y renombrar variables\nRestablecemos el índice para crear un índice limpio y secuencial y movemos la variable objetivo al final se realiza por buena práctica por razones de organización.\n\nAhora vamos ahora a definir un diccionario para definir nombres más directos e intuibles para las columnas, renombrar las columnas mejora la legibilidad y la interpretabilidad del conjunto de datos, facilitando a los usuarios la comprensión del significado de cada característica y a la presentación de resultados.\n\n- Monto préstamo\n  - **loan_amnt**: 'monto'\n\n- Ingresos anuales reportados\n  - **annual_inc**: ingreso_anual\n\n- ¿Son sus ingresos verificables?\n  - **verification_status**: ingreso_verificado\n\n- ¿Pagará en 36 meses ó en más (60 meses la otra opción)?\n  - **term_ 36 months**: pago_36_meses\n  - **term_ 60 months**: pago_60_meses\n\n- Duración del empleo en años. Van desde menos de un año hasta 10 años o más, también (incluye en formulario los casos en los que no se da el dato)\n\n  - **emp_length_1 year**:  emp_1_y\n  - **emp_length_10+ years**: emp_+10y\n  - **emp_length_2 years**:   emp_2_y\n  - **emp_length_3 years**:   emp_3_y\n  - **emp_length_4 years**:   emp_4_y\n  - **emp_length_5 years**:   emp_5_y\n  - **emp_length_6 years**:   emp_6_y\n  - **emp_length_7 years**:   emp_7_y\n  - **emp_length_8 years**:   emp_8_y\n  - **emp_length_9 years**:   emp_9_y\n  - **emp_length_< 1 year**:  emp_0_y\n  - **emp_length_n/a**: emp_n/a\n\n- Categorías de tipo propiedad casa\n  - **home_ownership_MORTGAGE**: casa_hipotecada\n  - **home_ownership_OWN**: casa_propia\n  - **home_ownership_RENT**: casa_rentada\n  - **home_ownership_OTHER**: casa_otra\n\n- Categorías próposito\n  - **purpose_car**: prop_carro                                    # Carro\n  - **purpose_credit_card**: prop_credito                          # Tarjeta de crédito\n  - **purpose_debt_consolidation**: prop_consolidacion             # Consolidar débito\n  - **purpose_educational**: prop_educativo                        # Educativo\n  - **purpose_home_improvement**: prop_mejorar_casa                # Mejorar Casa\n  - **purpose_house**: prop_comprar_casa                           # Comprar Casa\n  - **purpose_major_purchase**: prop_compra_imp                    # Compra importante\n  - **purpose_medical**: prop_salud                                # Salud\n  - **purpose_moving**: prop_mover                                 # Mudanza\n  - **purpose_renewable_energy**: prop_energia                     # Energía renovable\n  - **purpose_small_business**: prop_micro                         # Microempresa\n  - **purpose_vacation**: prop_vacaciones                          # Vacaciones\n  - **purpose_wedding**: prop_boda                                 # Boda\n  - **purpose_other**: prop_otro                                 # Otro\n\n- No pago del préstamo\n  - **loan_status**: no_pago","block_group":"6acbc2493c16442fa91809c9b0c43f69"},{"cellId":"00191528f38743caad7506edae152528","cell_type":"markdown","metadata":{"id":"IozTo5DOwA8z","deepnote_app_block_order":19,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"00191528f38743caad7506edae152528","deepnote_cell_type":"markdown"},"source":"# Analisis Descriptivo e Hipotesis","block_group":"d10cc1a2010147688461fa98d51db396"},{"cellId":"28444167622b4c1ba61b9a2406f02258","cell_type":"markdown","metadata":{"deepnote_app_block_order":20,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"28444167622b4c1ba61b9a2406f02258","deepnote_cell_type":"markdown"},"source":" El análisis del riesgo crediticio es una herramienta fundamental para las entidades financieras, ya que permite evaluar la probabilidad de que un solicitante de préstamo incumpla con sus obligaciones de pago. En este estudio se realiza un análisis descriptivo de una base de datos de préstamos, con el objetivo de identificar patrones y variables que influyen en el comportamiento de pago de los clientes.\n\n Se ha utilizado como variable objetivo loan_status, que distingue entre préstamos pagados e impagos. A partir de ella, se examinaron factores financieros y personales como la tasa de interés, el nivel de endeudamiento (dti), los ingresos anuales, y la calificación crediticia (grade), entre otros. Este enfoque permite obtener una visión general de los perfiles de mayor riesgo, proporcionando una base sólida para futuras decisiones de crédito y modelización predictiva.","block_group":"4a208f9fc90e422faf224db1205e352a"},{"cellId":"6a3bc705b86d48eabe1c9ac034c6b20f","cell_type":"markdown","metadata":{"deepnote_app_block_order":21,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"6a3bc705b86d48eabe1c9ac034c6b20f","deepnote_cell_type":"markdown"},"source":"##  Variable objetivo para el análisis: loan_status\n\n* Esta variable representa el estado del préstamo y es nuestro objetivo de análisis. Suele estar codificada como:\n\n* 0 = préstamo pagado correctamente\n\n* 1 = préstamo impagado o en mora\n\n","block_group":"869fbeca9691404bb667fb304589771c"},{"cellId":"8ea104374c9347fb80b0520a3c086187","cell_type":"markdown","metadata":{"deepnote_app_block_order":22,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"8ea104374c9347fb80b0520a3c086187","deepnote_cell_type":"markdown"},"source":"## Análisis de las variables predictoras \n\n\n### 1. loan_status\n* Tipo: categórica binaria (0 = pagado, 1 = impago)\n\n* Uso: Es la variable principal del análisis.\n\n* Gráfico de barras para visualizar la distribución de préstamos pagados vs impagos.\n\n¿Por qué se usa?\nPermite entender si hay un desbalance en la base de datos (por ejemplo, muchos más préstamos pagados que impagos), lo que es fundamental para diseñar modelos predictivos o estrategias de gestión de riesgo.\n\n<center>\n  <img src=\"images_report/loan_status.png\">\n  <i>Fig 1. Distribución del estado de prestamos(loan_status</i>\n</center>\n\n### 2. int_rate (Tasa de interés)\n* Tipo: numérica continua\n\n* Gráfico: Boxplot comparando la tasa de interés entre préstamos pagados e impagos.\n\n¿Por qué se usa?\nLa tasa de interés está directamente relacionada con el riesgo percibido por la entidad financiera. Normalmente, los préstamos con mayor riesgo se otorgan con tasas más altas. Analizar si los impagos tienden a tener mayores tasas permite validar si esta percepción se cumple en la práctica.\n\n<center>\n  <img src=\"images_report/Tasa_interes.png\">\n  <i>Fig 2. tasa de interes segun prestamo</i>\n</center>\n\n### 3. dti (Debt-to-Income Ratio)\n* Tipo: numérica continua\n* Gráfico: Distribución de densidad (KDE) comparando los valores de DTI entre los dos estados de préstamo.\n\n¿Por qué se usa?\nEl DTI mide qué proporción de los ingresos del solicitante está comprometida con deudas. Valores altos indican que el prestatario ya está financieramente comprometido, lo cual aumenta el riesgo de impago.\n\n<center>\n  <img src=\"images_report/Distribucion DTI.png\">\n  <i>Fig 3. Distribucion</i>\n</center>\n\n### 4. grade (Grado de crédito asignado por la entidad)\n* Tipo: categórica ordinal (de A a G, siendo A mejor y G peor)\n* Gráfico: Barplot con el porcentaje de impagos por cada grado.\n\n¿Por qué se usa?\nEl sistema de grados es una clasificación interna de riesgo. Este análisis nos permite verificar si realmente existe una correlación entre el grado otorgado y el comportamiento real de pago. Un aumento claro en el porcentaje de impagos a medida que el grado baja (de A a G) valida la calidad de esta evaluación interna.\n\n<center>\n  <img src=\"images_report/Grado_credito.png\">\n  <i>Fig 3.impagos por grado de creditos </i>\n</center>","block_group":"0bddfad5911446b4ad919a8a5a0da2dc"},{"cellId":"e16cb02d45e44614a8fcea5d641cf10f","cell_type":"markdown","metadata":{"deepnote_app_block_order":23,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"e16cb02d45e44614a8fcea5d641cf10f","deepnote_cell_type":"markdown"},"source":"## Conclusión de esta sección\n\nEl análisis descriptivo del riesgo crediticio ha permitido identificar varios factores clave asociados al impago de préstamos. Se observó que los solicitantes con tasa de interés más alta (int_rate), mayor nivel de endeudamiento (dti) y uso elevado del crédito rotativo (revol_util) presentaban tasas de impago significativamente superiores.\n\nAsimismo, el análisis por grado crediticio (grade) mostró un aumento progresivo del riesgo: los prestatarios con calificaciones más bajas (como F y G) alcanzaron porcentajes de impago superiores al 30%, en contraste con los grados A y B, que se mantuvieron por debajo del 10%. Esto confirma la validez del sistema interno de clasificación crediticia. Además, el propósito del préstamo también influyó en el riesgo: los préstamos destinados a consolidación de deudas y negocios pequeños concentraron los mayores niveles de impago.","block_group":"5fb39075fbc84ff0a4aa92012aa8c3c5"},{"cellId":"c1bb0f4b0d714cd2967637caecfcf773","cell_type":"markdown","metadata":{"id":"fC9mPZA6wIqY","deepnote_app_block_order":24,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"c1bb0f4b0d714cd2967637caecfcf773","deepnote_cell_type":"markdown"},"source":"# Modelos\nPara el desarrollo de modelos predictivos se empleó las interfaz de alto nivel proporcionada por la librerías Keras, y usando PyTorch como backend.\n\nAntes de empezar a entrenar modelos es importante definir métricas acorde a la naturaleza de este problema de clasificación de personas como potenciales no pagadores de crédito. Además utilizar alguna estrategía para tratar el fuerte desbalanace de clases evidenciado en la anterior sección.","block_group":"0bc1f5097c0b49ce8d8c04eb19ea12f7"},{"cellId":"566cc0794f504cdd93a9ed91409b380b","cell_type":"markdown","metadata":{"id":"wWD3hBvI9dO-","deepnote_app_block_order":25,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"566cc0794f504cdd93a9ed91409b380b","deepnote_cell_type":"markdown"},"source":"## Definicion de Metricas Claves\n\nEste problema de clasificación binaria trata sobre predecir en base a la probabilidad de impago retornada por un modelo, si una persona que va solicitar crédito califica como pagador o no pagador. La variable objetivo se codificó como **1 para un cliente que no pagó**, por lo tanto lo ideal es tener una tasa de detección de verdaderos positivos muy alta (True Positive Rate), dicha métrica también se conoce como *Recall* o *Sensitivity*. Esta será la métrica clave ya que el dinero que se pierde por no prestarle a un cliente que no pagará, es mucho mayor que el que se pierde por no prestarle a un cliente que si pagará.\n\nLa métrica **Recall** se define como:\n\n$$\n\\text{Recall} = \\frac{TP}{TP + FN} \\quad (1)\n$$\n\nComo se muestra en la ecuación (1), el **Recall** mide la proporción de verdaderos positivos (TP) frente a los falsos negativos (FN).\n\n\nA parte de esto, también vamos a usar como métrica el **AUC PR** para comparar los modelos entre sí, para ver el mejor clasificador, esta es mejor que la **AUC ROC**, ya que el **AUC PR** determina el área bajo la curva entre el **Precision** y el **Recall** para diversos umbrales, dicha métrica se compara respecto a un *baseline* determinado por la proporción de muestras de la variable objetivo **Y = 1** (no pagadores), así pues es mucho mejor indicador en problemas desbalanceados.\n\nLa escogencia entonces del modelo de despliegue estará sujeta a una comparativa de:\n\n- Recall/Sensitivity (True Positive Rate)\n- AUC PR (área bajo la curva PR)\n- AUC ROC (área bajo la curva ROC)\n- Accuracy (exactitud general de clasificación)","block_group":"ea9f36c155f140e69e6d70d343003af1"},{"cellId":"59274dc7c3cd40518fca2cee636278b9","cell_type":"markdown","metadata":{"id":"oe2w7V9W-aI2","deepnote_app_block_order":26,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"59274dc7c3cd40518fca2cee636278b9","deepnote_cell_type":"markdown"},"source":"## Tratamiento del Desbalance\n\nSiendo un desbalance claro como se vió en la sección de análisis, con aproximadamente 78.09% de los datos siendo de la clase 0 (si pagó), y 21.90% para la clase 1 (no pagó); es necesario abordar técnicas para tratar este desbalance de clases.\nAquí la estrategia elegida fue usar una técnica de ponderación de la función de pérdida, para indicarle a los modelos que presten atención a las muestras de la clase subrepresentada. Esta se implementó fácilmente con la función `class_weight` de la librería Scikit-Learn, la cuál por medio de las etiquetas, estudia su proporción y calcula los factores de ponderación para la función de pérdida.","block_group":"6603930d646b4f86a83774ab5c1e5417"},{"cellId":"adab4e854f12447984d188dd494d2550","cell_type":"markdown","metadata":{"id":"qSST8ZjtAqAP","deepnote_app_block_order":27,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"adab4e854f12447984d188dd494d2550","deepnote_cell_type":"markdown"},"source":"## Funcion de Perdida\nElegimos como función de pérdida la entropía binaria cruzada por su idoneidad natural para problemas de clasificación binaria cuando se combina con la activación sigmoide en la capa de salida. Esta función está fundamentada en principios de máxima verosimilitud, ya que modela la salida de la red como una distribución de Bernoulli, donde cada predicción representa la probabilidad de que una muestra pertenezca a la clase positiva. La BCE penaliza de manera asimétrica las predicciones incorrectas, siendo más severa con aquellas predicciones que muestran alta confianza pero son erróneas, lo cual es deseable desde el punto de vista del aprendizaje.\nAdemás, la BCE presenta propiedades matemáticas óptimas para el entrenamiento de redes neuronales: es convexa cuando se usa con la activación sigmoide, garantizando la existencia de un mínimo global, y sus derivadas son suaves y bien comportadas, facilitando la convergencia del algoritmo de optimización. Su implementación es computacionalmente eficiente y numéricamente estable, evitando problemas comunes como el desvanecimiento del gradiente que pueden surgir con otras funciones de pérdida en este tipo de problemas.\n\n$$\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\quad (2)$$\n\nDonde:\n\n- $N$ es el número de muestras\n\n- $y_i$ es la etiqueta verdadera (0 ó 1) para la muestra\n\n- $\\hat{y}_i$ es la probabilidad predicha para la muestra","block_group":"f7aebea8bc5046f2b8b88fdd4e047574"},{"cellId":"110aba1c929e40e6af405d5ec2e11f53","cell_type":"markdown","metadata":{"id":"K2v-1VyJCI7Y","deepnote_app_block_order":28,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"110aba1c929e40e6af405d5ec2e11f53","deepnote_cell_type":"markdown"},"source":"## Estrategia de Entrenamiento\nPara el entrenamiento se emplearon diversas estrategias variadas, unas se aplicaron a unos modelos y otras no. Entre todas estas estrategias aplicadas a lo largo del testeo de modelos están:\n\n- Diferentes learning rates.\n- **Optimizador Adam** porque cambia automáticamente de la tasa de aprendizaje por parámetro, eliminando la necesidad de ajuste manual extensivo.\n- Implementar **split de datos en el entrenamiento, para tener un subconjunto de validación**, esto de forma aleatoria en cada época, para así **cumplir con la condición de evaluar el modelo con múltiples conjuntos de datos**.\n- Uso de **Model CheckPoints** para guardar mejores parámetros en una época según los resultados en el split de validación.\n- Uso de **Early Stoppings** para detener el entrenamiento cuando según alguna métrica, el modelo no ha mejorado tras cierta cantidad de épocas.\n- Capas de **Dropout** como estrategia de regularización y añadir ruido al entrenamiento para quizás mejorar el desempeño con datos con diferentes distribución y/o evitar el overfitting.\n- Capas de **Batch Normalization** para estabilizar y acelerar el entrenamiento. Esto reduce el desplazamiento de covariables internas, lo que permite usar tasas de aprendizaje más altas y hace que el entrenamiento sea menos sensible a la inicialización de pesos.","block_group":"9edf04647e06493c95527ead38b55a16"},{"cellId":"3c3e39c7b51c40abbdd3e73d3bc9976f","cell_type":"markdown","metadata":{"id":"YxUg7KskCUr1","deepnote_app_block_order":29,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"3c3e39c7b51c40abbdd3e73d3bc9976f","deepnote_cell_type":"markdown"},"source":"## Perceptrones Simples como Referencia\nUna buena práctica es siempre iniciar con modelos de baja complejidad para tener una referencia o pistas que permitan trazar un camino de hacia donde orientar el diseño de arquitectura.\nPara esto se van a entrenar 2 perceptrones con diferentes learning rate (uno con 0.001 y otro con 0.01), y ambos van usar Model CheckPoint para guardar los pesos de la época con el mejor **Recall en el split de validación**. Dando entonces en total 4 modelos diferentes de perceptrones simples.\n\nLa arquitectura de todos es la misma, una capa de entrada que recibe las características escaladas y codificadas, con una única neurona de salida con función de activación sigmoide para obtener la probabilidad.\n\nVeamos entonces los resultados de estos modelos para el conjunto de prueba de los datos.","block_group":"cd87c81f834f4ad89a05cd399170ab6b"},{"cellId":"c0dd8e7764104160b33aebcfc0e7e972","cell_type":"markdown","metadata":{"id":"5Y-pEFw4Okzo","deepnote_app_block_order":30,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"c0dd8e7764104160b33aebcfc0e7e972","deepnote_cell_type":"markdown"},"source":"<center>\n<img src=\"images_report/comparativa_metricas_perceptrones.png\">\n<i>Fig. 4 Comparativa Métricas Perceptrones<i>\n</center>","block_group":"706147386c134d7e8d8399e6456e7fb6"},{"cellId":"5cba8c39178a49b2956411e9f9a934a7","cell_type":"markdown","metadata":{"id":"vazSfqQDO6An","deepnote_app_block_order":31,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"5cba8c39178a49b2956411e9f9a934a7","deepnote_cell_type":"markdown"},"source":"Vemos en general que los modelos con checkpoint tuvieron el mejor desempeño en cuanto a la métrica de recall, aunque no necesariamente fueron los mejores en accuracy, por lo cuál aquí se evidencia el trade-off entre estas 2 métricas en un problema desbalanceado. Ahora veamos las curvas ROC de los modelos\n\n<center>\n<img src=\"images_report/curvas_roc_perceptrones.png\">\n<i>Fig. 5  Curvas ROC Perceptrones<i>\n</center>\n\nViendo esto incluyendo el AUC ROC podríamos pensar en inclinarnos por lo modelos de Perceptron Tipo 2 (learning rate de 0.01), pero en problemas desbalanceados donde el objetivo es detectar mejor la clase submuestreada, esta métrica no es la mejor.\n\nPasamos ahora a ver las curvas PR (Precission - Recall) ya que estas son más apropidas para clasificadores con datos desbalanceados.\n\n<center>\n<img src=\"images_report/curvas_pr_perceptrones.png\">\n<i>Fig 6 Curvas PR Perceptrones<i>\n</center>","block_group":"c27c819b7c4d4baba8db519c9abdfcea"},{"cellId":"e0f171c9a9924ddea28b1afb5653d997","cell_type":"markdown","metadata":{"deepnote_app_block_order":32,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"e0f171c9a9924ddea28b1afb5653d997","deepnote_cell_type":"markdown"},"source":"Tras analizar los resultados de los 4 perceptrones, y considerar que métricas como **AUC ROC** y **AUC PR** indican en general que tan bueno es el clasificador (para todos los umbrales), la elección del perceptrón de referencia final, está sujeta a eso, siendo entonces que los perceptrones tipo 2 (learning rate de 0.01), tienen mejor puntaje en este caso, por lo cual se seleccionará entre los de este tipo.\nAhora bien, entre estos 2, el perceptrón con parámetros guardados por checkpoint en la mejor época es el que tiene mejor *recall (true positive rate)*, la cual es la métrica clave. Por tal motivo:\n\n**Perceptrón seleccionado como modelo de referencia**: Perceptrón 2 Checkpoint.","block_group":"768ea44a56c6407d95508cb8333bf2b2"},{"cellId":"11bc99f80a7f404f9df8e96232133724","cell_type":"markdown","metadata":{"id":"RcZvPDILQHD3","deepnote_app_block_order":33,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"11bc99f80a7f404f9df8e96232133724","deepnote_cell_type":"markdown"},"source":"## Redes Neuronales Sencillas\n\nAquí se plantearon arquitecturas más complejas con el ánimo de captar mejor el comportamientos de los datos de entrenamiento.\n\nLa arquitectura es de 3 capas ocultas con función de activación **LeakyReLU** en cada neurona, la razón de esta función de activación es para poder aprovechar los valores negativos en los datos de entrenamiento, cosa que nos perderiamos si se usará la conocida **ReLU**, ya que los valores negativos los convierte a 0, por lo que no contribuirán a la red en un proceso de actualización de pesos según el gradiente.\n\nPor otro lado la cantidad de neuronas por capa siguen una estrategia de arquitectura piramidal o \"embudo\", siguiendo un flujo de número de neuronas así:\n\n**n_features $\\rightarrow$ 2 $\\cdot$ n_features $\\rightarrow$ n_features $\\rightarrow$ n_features/2 $\\rightarrow$ 1**\n\nTeniendo al final una función de activación sigmoide con una única neurona para obtener las probabilidades.\n\nSe plantearon 2 variaciones de esta arquitectura, una **sin Dropout** y la otra **con capas de Dropout de 0.2**\n\nAdemás, aquí se repite de nuevo la estrategia de utilizar Model CheckPoints para guardar los parámetros de la red en la época con **mejor Recall en el split de validación**, dando así de nuevo un total de 4 modelos, donde los denotados con la etique *Full* son aquellos con los pesos entrenados hasta la última época, mientras lo de *Checkpoint* son los pesos de la red que se han guardado en la mejor época.\n\nVeamos los resultados en las métricas de los 4 modelos.\n\n<center>\n<img src=\"images_report/comparativa_metricas_redes.png\">\n<i>Fig. 7 Comparativa Métricas Redes<i>\n</center>","block_group":"d367fee385984f63b6075d0cbf863a7e"},{"cellId":"fb342c0feb6c445fadf16de3437e8506","cell_type":"markdown","metadata":{"deepnote_app_block_order":34,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"fb342c0feb6c445fadf16de3437e8506","deepnote_cell_type":"markdown"},"source":"Según estas primeras gráficas, el modelo de *Red 2 Checkpoint* pareciera una buena opción por tener el **Recall** más alto. Sin embargo en otras métricas como el **Accuracy** se queda mucho más pobre. Veamos gráficamente las curvas ROC y PR para terminar de elegir un modelo de red neuronal entre estos 4.\n\n<center>\n<img src=\"images_report/curvas_roc_redes.png\">\n<i>Fig. 8 Curvas ROC Redes<i>\n</center>\n\n<center>\n<img src=\"images_report/curvas_pr_redes.png\">\n<i>Fig. 9 Curvas PR Redes<i>\n</center>","block_group":"8a2f6debf60b49b2a106b0417452cead"},{"cellId":"2e7b523f74514a57b7e6cfdea0b52e32","cell_type":"markdown","metadata":{"deepnote_app_block_order":35,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"2e7b523f74514a57b7e6cfdea0b52e32","deepnote_cell_type":"markdown"},"source":"Tras observar y tener mejor noción de los modelos en acompañamiento de las curvas de desempeño **ROC y PR**, se observa que el mejor desempeño lo tienen las redes neuronales tipo 1 (sin dropout), y en particular entre estas 2 redes tipo 1, empatan en estos scores **AUC ROC** y **AUC PR**; por lo que el desempate se determina según **Recall** con más peso, y **Accuracy** de forma secundaria. Por lo que el modelo elegido es:\n\n**Red Neuronal seleccionada como modelo de referencia**: Red 1 Checkpoint (entrenada sin dropout).","block_group":"44d9b85976ed46eb9d2c6287806fdd71"},{"cellId":"1b18f3c215294e6a9ae6bd454941f455","cell_type":"markdown","metadata":{"deepnote_app_block_order":36,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"1b18f3c215294e6a9ae6bd454941f455","deepnote_cell_type":"markdown"},"source":"## Red con Batch Normalization y Dropout","block_group":"4211f109988a4cdcbb1f7a3f37142a41"},{"cellId":"4b02b7ec163d47ac81f45415e8fb447c","cell_type":"markdown","metadata":{"deepnote_app_block_order":37,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"4b02b7ec163d47ac81f45415e8fb447c","deepnote_cell_type":"markdown"},"source":"Tras analizar los resultados de los anteriores modelos, se opta por entrenar otra red neuronal con la misma estrategia de *# neuronas* por capa dando una forma de embudo, pero **con Batch Normalization** y **con Dropout**, s a la vez que se cambian las condiciones de entrenamiento de la siguiente forma:\n\n- **Learning Rate**: 0.01 $\\rightarrow$ 0.001\n- **Batch Size**: 64 $\\rightarrow$ 32\n- **Monitoreo**: Model Checkpoint $\\rightarrow$ Early Stopping.\n\nLa razón del Batch Normalization (BN) para datos tabulares es porque estabiliza el entrenamiento al mitigar el *Internal Covariate Shift*, problema donde la distribución de las activaciones de entrada a cada capa cambia constantemente durante el aprendizaje. Según Ioffe y Szegedy (2015), al normalizar estas activaciones dentro de cada mini-lote (haciendo que tengan una media cercana a cero y una varianza cercana a uno), BN acelera significativamente la convergencia al permitir tasas de aprendizaje más altas, mejora la estabilidad al reducir la probabilidad de gradientes evanescentes o explosivos, y actúa como un regularizador que ayuda a prevenir el sobreajuste\n\nEn cuanto al Batch Size es hace más pequeño, lo cual aunque aumenta los tiempos de entrenamiento, permite actualizaciones de pesos con menor cantidad de datos, tendiendo un poco más que con 64 a lo que se llama aprendizaje online.\n\nSe establece un entrenamiento de 50 épocas, pero en lugar de usar un Model CheckPoint con el mejor Recall en validación, se usará un Early Stopping que tras 5 épocas sin mejorar la pérdida, detiene el entrenamiento y se queda con los mejores parámetros.\n\nVeamos entonces los resultados de este único modelo\n\n<center>\n<img src=\"images_report/metricas_red_batch.png\">\n<i>Fig. 10 Comparativa Métricas Red BN + Dropout<i>\n</center>","block_group":"db664ee51d97414c8235bd429f00a89e"},{"cellId":"9e196909b3a8460d84642eead681765c","cell_type":"markdown","metadata":{"deepnote_app_block_order":38,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"9e196909b3a8460d84642eead681765c","deepnote_cell_type":"markdown"},"source":"Ahora sus curvas ROC y PR:\n\n<center>\n<img src=\"images_report/curvas_red_batch.png\">\n<i>Fig. 11 Curvas Red BN + Dropout<i>\n</center>","block_group":"570477718dd348fdbb6fce2a0ea0cf65"},{"cellId":"44ee059d49664145aa9579bbb44eb00e","cell_type":"markdown","metadata":{"deepnote_app_block_order":39,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"44ee059d49664145aa9579bbb44eb00e","deepnote_cell_type":"markdown"},"source":"Según los puntajes (AUC) ROC y PR, este modelo no lo hace mal, faltaría entonces intentar cambiar el umbral por defecto de 50% de probabilidad, a ver como cambian sus métricas en comparativa con el umbral para los demás modelos elegidos.","block_group":"7a9c03da3d7940c4a8477ee3415e6f0e"},{"cellId":"cb87fe5a45be4ffbb413c53bf6bdc15f","cell_type":"markdown","metadata":{"deepnote_app_block_order":40,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"cb87fe5a45be4ffbb413c53bf6bdc15f","deepnote_cell_type":"markdown"},"source":"## Seleccion del Umbral de Corte y Modelo\n\nAhora se toman los 3 modelos escogidos (Perceptrón, Red Neuronal, Red Neuronal + Batch Normalization) y se va llevar los 3 modelos al umbral en el que tengan un 80% de **Recall**, la cuál es la métrica clave. Una vez llevados al umbral en el que cada uno logra eso, compararemos con las demás métricas.\n\nLa razón de este umbral de corte para obtener un *Recall* del 80% es para tener una cobertura del 80% (ampliamente mayoritaria) de las clases submuestreadas de tipo 1 (no pagadores de crédito). Esto se justifica entendiendo que conceder cŕedito a un mal pagador es mucho peor que no conceder crédito a un buen pagador, es un trade-off que se aplica al modelo para que sus predicciones sean más seguras alrededor del tema de financiación de cŕedito.\n\nPara esto entonces se toman los 3 modelos y se les aplica un procedimiento que busca de forma iterativa el mínimo umbral de probabilidad posible, tal que el modelo pueda obtener un 80% de **Recall** con los datos de test. Veamos los resultados obtenidos por este procedimiento:\n\n<center>\n<img src=\"images_report/tabla_umbrales.png\">\n<i>Fig. 12 Umbrales Obtenidos por Modelo<i>\n</center>","block_group":"b735d1a1dd9847a3a4a98f5f677860a9"},{"cellId":"bbc635824eeb4ac29782ef1a676437c8","cell_type":"markdown","metadata":{"deepnote_app_block_order":41,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"bbc635824eeb4ac29782ef1a676437c8","deepnote_cell_type":"markdown"},"source":"Vemos que para llegar a un Recall de 0.8 (80%), el modelo de red neuronal con batch normalization y dropout requirió bajar un poco más su umbral, esto en princpio podría sonar como si fuera el peor modelo, sin embargo es a tráves de las métricas que se puede dar un mejor veredicto. Observemos a continuación las nuevas métricas por modelo con los umbrales obtenidos:\n\n<center>\n<img src=\"images_report/metricas_umbrales.png\">\n<i>Fig. 13 Comparativa Metricas con Umbrales Nuevos<i>\n</center>","block_group":"634789eeb47f47fb998b641fc4fa94c3"},{"cellId":"57923f4a8fce4b389df9b06e275740c9","cell_type":"markdown","metadata":{"deepnote_app_block_order":42,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"57923f4a8fce4b389df9b06e275740c9","deepnote_cell_type":"markdown"},"source":"Se puede ver rápidamente que aunque el umbral más bajo parecia dar señal de que el modelo con batch normalization y dropout no era el mejor, vemos que en comparativa considerando la métrica de *Accuracy*, el modelo con BN +dropout es el mejor. Permite con un Recall de 80% en igualdad de condiciones con los otros 2 modelos, obtener mejores predicciones en general. Además para despejar cualquier duda, sus métricas de score ROC y PR son mayores, indicando que es no solo un mejor clasificador en general, sino que también es mejor clasificador para escenarios de desbalance.\n\nApreciemos esto también por medio de las matrices de confusión para cada modelo.\n\n<center>\n<img src=\"images_report/matriz_confusion_modelos.png\">\n<i>Fig. 14 Matrices de Confusión<i>\n</center>","block_group":"0a5194b761544af1b2135acb9769634c"},{"cellId":"88bc9cff911e4532a60002b86b44b6bd","cell_type":"markdown","metadata":{"deepnote_app_block_order":43,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"88bc9cff911e4532a60002b86b44b6bd","deepnote_cell_type":"markdown"},"source":"Por medio de las matrices de confusión se puede apreciar mucho mejor la superioridad del modelo de redes neuronales con batch normalization y dropout, aunque todos los modelos castigan a muchos usuarios del conjunto de test como no pagadores del crédito cuando si son pagadores, el modelo mencionado lo hace en menor cantidad, de ahí que su superioridad en las otras métricas como **Accuracy** y **Precisión** sea mucho mejor que el modelo de perceptrón simple y de red neuronal sencilla.\n\n**Modelo Final Elegido**: Red Neuronal con Batch Normalization y Dropout.","block_group":"08188a6bbdbb43b4bd3a6822e8893f55"},{"cellId":"b531e543efba4f96987240b10544c00c","cell_type":"markdown","metadata":{"deepnote_app_block_order":44,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"b531e543efba4f96987240b10544c00c","deepnote_cell_type":"markdown"},"source":"## Scorecard para el Modelo Final","block_group":"0e6fc7458ebe4273ad8078641e972f05"},{"cellId":"dbc8c0c713a44aa0ba9b9ff77079c47e","cell_type":"markdown","metadata":{"deepnote_app_block_order":45,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"dbc8c0c713a44aa0ba9b9ff77079c47e","deepnote_cell_type":"markdown"},"source":"Ahora bien, ya con un modelo formulado y un umbral optimizado para un **Recall de 80%**. Lo siguiente es llevar los resultados a una representación de score crediticio, para esto tomamos como referencia lo establecido por Thomas, Edelman y Crook (2002) en su publicación sobre scores de crédito. Estableciendo que la formulación para el cálculo de un scorecard sigue la siguiente ecuación.\n\n$$\n\\text{Score} = \\text{Offset} + \\text{Factor} \\cdot \\ln\\left( \\frac{1 - p}{p} \\right) \\quad (3)\n$$\n\nLa ecuación (3) está dada por:\n- $p$ es la probabilidad estimada de incumplimiento (output del modelo)\n- $\\text{Offset}$ y $\\text{Factor}$ son parámetros de la fórmula que a su vez se calculan como:\n\n  $$\\text{Factor} = \\frac{\\text{PDO}}{\\ln(2)}$$\n\n  $$\\text{Offset} = \\text{Score}_0 - \\text{Factor} \\cdot \\ln(\\text{Odds}_0)$$\n\nEstos otros nuevos parámetros internos, con su respectivo valor estándar establecido por Thomas, Edelman y Crook (2002) son:\n\n- $\\text{Odds}_0$: Cantidad de buenos pagadores que se esperan por un solo mal pagador. Establecido en la publicación como **50**.\n- $\\text{PDO}$: Cantidad de puntos que debe aumentar el score para que las odds (probabilidad de que pague sobre que no pague) se dupliquen. Usalmente con un valor de **50**.\n- $\\text{Score}_0$: Puntaje (score) que se asigna a una probabilidad de incumplimiento base específica (a partir de unas odds base). Con un valor de **600** estándar.\n\nCon todo esto, es posible realizar el cálculo a partir de las probabilidades retornadas por el modelo. Para esto simplemente se define una función en Python que realice el cálculo según las probabilidades, y con dicha función aplicada a la probabilidad de corte del modelo, se encuentra el score crediticio de corte, el cual al calcularlo se obtiene un valor de:\n\n**$$\\text{Scorecard de Corte}\\approx \\text{346.257 puntos}$$**","block_group":"c454ee13c20d425792073bd65376f79a"},{"cellId":"418c92634e934bc294ce3b0398f88bc4","cell_type":"markdown","metadata":{"deepnote_app_block_order":46,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"418c92634e934bc294ce3b0398f88bc4","deepnote_cell_type":"markdown"},"source":"Veamos entonces, como se ve el histograma normalizado de scores crediticios de los datos de test, obtenido por medio de las probabilidades calculadas con el modelo, indicando el punto de corte, y los lados de aprobación y rechazo. Entendiendo pues, que a mayor score crediticio, más susceptible es el usuario de obtener su crédito solicitado.\n\n<center>\n<img src=\"images_report/histograma_scorecard.png\">\n<i>Fig. 15 Scorecard de Corte<i>\n</center>","block_group":"d6ae05750a00485f830f724793a72d01"},{"cellId":"50c8b4f023254a2da4518dbdfa3cb8d3","cell_type":"markdown","metadata":{"cell_id":"50c8b4f023254a2da4518dbdfa3cb8d3","deepnote_cell_type":"markdown"},"source":"Podemos ver en el histograma que la línea de corte del modelo muestra un cambio en la distribución de los scores inmediato, esto apoya gráficamente el punto de corte del puntaje crediticio. Aunque, es importante entender que por el trade-off del modelo para mejorar la detección de malos pagadores, hay algunos buenos pagadores que pueden estar en la región de susceptible a rechazo de crédito, reflejando así el problema real en el mundo del crédito, donde los malos pagadores afectan las posibilidades de acceso de crédito de personas que serían buenos pagadores.\n\nCon todo esto claro, pasemos conocer a detalle el despliegue e implementación del modelo crediticio en la aplicación web.\n\n**NOTA:** El listado de aprendizajes de toda esta sección de modelamiento será sintetizado al final en las conclusiones del reporte\n\n<br>\n\n---\n","block_group":"c501f1d65e244d84a56678188166dcec"},{"cellId":"55810f4a985e4e17a4b4418d74b11c53","cell_type":"markdown","metadata":{"deepnote_app_block_order":47,"deepnote_app_block_visible":true,"deepnote_app_block_group_id":null,"cell_id":"55810f4a985e4e17a4b4418d74b11c53","deepnote_cell_type":"markdown"},"source":"# Aplicacion web\n\nAcceso a la aplicación: [https://web-production-717668.up.railway.app/](https://web-production-717668.up.railway.app/)\nLink alternativo: [https://credit-risk-bc78.onrender.com/](https://credit-risk-bc78.onrender.com/)\n\n---\n\n## Arquitectura General\n\nSe implementó una arquitectura monolítica sencilla utilizando **Flask**, un microframework de Python ideal para aplicaciones ligeras. El flujo de procesamiento de datos sigue esta secuencia:\n\n1. **Entrada**: Formulario en HTML\n2. **Validación**: Lado cliente (frontend) y servidor (backend)\n3. **Preprocesamiento**: Codificación + escalado manual\n4. **Predicción**: Utilizando ONNX Runtime\n5. **Respuesta**: Score (Aprobado / Rechazado)\n\n---\n\n## Tecnologías Utilizadas\n\nLa Tabla 1 presenta las tecnologías clave utilizadas en la implementación del sistema, junto con sus versiones y propósitos.\n\n**Tabla 1. Tecnologías utilizadas en el sistema**\n\n| Categoría       | Tecnología     | Versión  | Propósito                         |\n|-----------------|----------------|----------|-----------------------------------|\n| Backend         | Flask          | 2.3.3    | Microframework web                |\n| ML Runtime      | ONNX Runtime   | 1.16.3   | Inferencia optimizada y ligera    |\n| Computación     | NumPy          | 1.24.3   | Operaciones numéricas             |\n| Servidor        | Gunicorn       | 21.2.0   | Servidor WSGI para producción     |\n| Hosting         | Render         | -        | Plataforma cloud gratuita         |\n\n---\n\n## Optimización de Recursos\n\nDebido a las limitaciones del hosting gratuito provisto por Render, fue necesario optimizar el uso de memoria. Para ello se reemplazaron y ajustaron tecnologías, como se resume en la Tabla 2.\n\n**Tabla 2. Tecnologías reemplazadas u optimizadas por limitaciones de entorno**\n\n| Tecnología         | Uso Original               | Estado Actual             |\n|--------------------|----------------------------|---------------------------|\n| TensorFlow / Keras | Entrenamiento del modelo   | Eliminado               |\n| Scikit-learn       | Preprocesamiento           | Reemplazado / Optimizado|\n| Joblib             | Serialización              | Reemplazado por JSON   |\n\n## Resultados de la Optimización\n\nGracias a estos cambios:\n\n- Se redujo el tamaño del proyecto en **más del 85%**.\n- Se eliminó la necesidad de cargar TensorFlow en producción.\n- El escalado se realizó manualmente sin necesidad de `scikit-learn`, lo que permitió una carga instantánea.\n- La inferencia es ahora más eficiente gracias a ONNX Runtime.\n\n---","block_group":"88ccb7a393704a369bd7f65d84d5074c"},{"cellId":"750a088f55e54ebbaa186b07bfdc3f1d","cell_type":"markdown","metadata":{"cell_id":"750a088f55e54ebbaa186b07bfdc3f1d","deepnote_cell_type":"markdown"},"source":"# Conclusiones\n\n- La preparación de los datos es un paso vital para entrenar modelos, es importante realizar siempre esta tarea de manera muy consciente y analítica, ya que hacerlo mal implica perder tiempo y procesamiento computacional, pues las malas métricas terminan saliendo a la luz, y exigen retroceder en el flujo de los datos para realizar las correciones pertinentes.\n\n- Hacer análisis descriptivos e hipótesis es muy importante, pues a raíz de esto se pueden obtener indicios de las variables con mayor peso e importancia en el problema particular, lo cual no solo enriquece previamente al modelo, sino también en el análisis consciente postmodelo.\n\n- El diseño de arquitecturas y uso de técnicas para entrenar redes neuronales es todo un mundo a parte en el *machine learning*, no solo es importante tener unos fundamentos claros y conceptuales de las redes neuronales artificiales, también la experticia en este campo puede hacer la diferencia entre gastar mucho tiempo, cómputo y por tanto dinero, para terminar teniendo modelos subóptimos; a lograr obtener modelos altamente eficientes con menos inversión.\n\n- Las métricas son el alma para evaluar modelos, quedarse con una o muy pocas métricas de evaluación es terrible, ya que en problemas donde hay desbalance o que es necesario un trade-off, usar solo métricas como el **accuracy** terminarán maquillando como buen modelo, a uno que realmente no lo es para el problema en particular.\n\n- Lograr establacer el puente entre las salidas abstractas de un modelo y la realidad económica, política y social; es clave para poder evaluar de forma realista y explicativa la solución. La comunicación y venta de soluciones es una habilidad fundamental para cualquier persona que se desempeñe como *problem solver*.\n\n- Este proyecto demuestra que es posible construir una aplicación de *machine learning* funcional, rápida y liviana con herramientas simples y eficientes, incluso en un entorno limitado como Render. El uso estratégico de librerías ligeras y la eliminación de dependencias pesadas permitieron cumplir los objetivos de rendimiento y accesibilidad.\n\n- En el proceso de entregar soluciones es importante la conexión y estratificación de tareas en el equipo de desarrollo, pues siempre hay que tener en mente que donde culmina el aporte indivdual no es el final del trabajo completo, sino el inicio de otra etapa importante que es clave para cumplir con los objetivos.","block_group":"3cf209d36df24ab88e6477f93e3abcbf"},{"cellId":"fde462563e854d3788996c12dad984a2","cell_type":"markdown","metadata":{"cell_id":"fde462563e854d3788996c12dad984a2","deepnote_cell_type":"markdown"},"source":"# Bibliografia\n\n- Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. Society for Industrial and Applied Mathematics.\n\n- Ullah, I., & Petrosino, A. (2016).*About Pyramid Structure in Convolutional Neural Networks*. arXiv. https://arxiv.org/abs/1608.04064  \n\n\n- Di Nardo, E., Petrosino, A., & Ullah, I. (2018). *EmoP3D: A Brain Like Pyramidal Deep Neural Network for Emotion Recognition*. En L. Leal-Taixé & S. Roth (Eds.), *Computer Vision – ECCV 2018 Workshops* (pp. 607–616). Springer. https://doi.org/10.1007/978-3-030-11015-4_46  \n\n- Chollet, F., et al. (s. f.). ModelCheckpoint. En Keras Documentation\n\n- Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*. *Journal of Machine Learning Research*, 15, 1929–1958.\n\n- Ioffe, S., & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*. En *Proceedings of the 32nd International Conference on Machine Learning* (pp. 448–456). https://proceedings.mlr.press/v37/ioffe15.html  \n\n- Xu, B., Wang, N., Chen, T., & Li, M. (2015). *Empirical Evaluation of Rectified Activations in Convolutional Network*. arXiv. https://arxiv.org/abs/1505.00853  \n\n- Prechelt, L. (1998). *Early Stopping — But When?*. En G. Dorffner, H. Bischof, & K. Hornik (Eds.), *Neural Networks: Tricks of the Trade* (pp. 55–69). Springer. https://link.springer.com/chapter/10.1007/3-540-49430-8_3  \n\n\n- TensorFlow. (2025). *ModelCheckpoint*. https://keras.io/api/callbacks/model_checkpoint/  \n\n- Calani, M. (2019). *Can Regulation on Loan-Loss-Provisions for Credit Risk Affect the Mortgage Market? Evidence from Administrative Data in Chile*. *BIS Working Papers No. 780*. https://www.bis.org/publ/work780.htm  \n","block_group":"64ec3038b1f845dcb94155eaf42ed1fb"}],
        "metadata": {"deepnote_persisted_session":{"createdAt":"2025-06-13T00:51:36.718Z"},"deepnote_notebook_id":"158640a3a85f44298d36060726ab20b8"},
        "nbformat": 4,
        "nbformat_minor": 0,
        "version": 0
      }